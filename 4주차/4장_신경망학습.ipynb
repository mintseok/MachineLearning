{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4장_신경망학습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTN4ZT/EYXdcwNKK3lUad0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mintseok/MachineLearning/blob/main/4%EC%A3%BC%EC%B0%A8/4%EC%9E%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXSmfWuH2pjr"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1fOGoso1yq4"
      },
      "source": [
        "# 오차제곱합 sum of squares for error, SSE\n",
        "def sum_squares_error(y, t):              #y, t는 모두 np.array 형식\n",
        "  return 0.5 * np.sum((y-t)**2)           #y는 예상, t는 답"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOaWw-dK2H7k"
      },
      "source": [
        "# 교차 엔트로피 오차 cross entropy error, CEE\n",
        "def cross_entropy_error(y, t):\n",
        "  delta = 1e-7                          #0.0000001 \n",
        "  return -np.sum(t*np.log(y+delta))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fci_r16v2xN5"
      },
      "source": [
        "# MiniBatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOIXlAnb_gy2",
        "outputId": "99d08683-d34d-4541-d711-eb850feaaf3e"
      },
      "source": [
        "!curl -O https://raw.githubusercontent.com/WegraLee/deep-learning-from-scratch/master/dataset/mnist.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3655  100  3655    0     0  58015      0 --:--:-- --:--:-- --:--:-- 58015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEDCTu332w9a",
        "outputId": "057b4e7c-4fe6-4cc9-b224-d2cf40ec9400"
      },
      "source": [
        "import sys, os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
        "\n",
        "print(x_train.shape) #(60000, 784)=> 784는 28x28\n",
        "print(t_train.shape) #(60000,)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7COQojGGNJc",
        "outputId": "aa39f634-390d-4710-c070-1bff250cc71c"
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size) #np.random.choice(60000, 10)은 60000 미만인 index에서 10개를 무작위로 추출\n",
        "x_batch = x_train[batch_mask]\n",
        "t_batch = t_train[batch_mask]\n",
        "#np.random.choice(train_size, batch_size)\n",
        "x_batch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUdspE1aINGa"
      },
      "source": [
        "#minibatch용 cross entropy loss function(for one_hot_encoding)            #어차피 minibatch로 model 만들꺼니까, 위에서 define한 함수 무시.\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t*np.log(y+1e-7)) / batch_size "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ICl-Se6KxBw"
      },
      "source": [
        "# 수치 미분"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igVEu4QZKctO"
      },
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4                                #0.0001\n",
        "  return (f(x+h) - f(x-h)) / (2*h)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2owfZkLLz7"
      },
      "source": [
        "# 기울기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-dvQZ0Zh0GZ"
      },
      "source": [
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        \n",
        "        # f(x+h) 계산\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)\n",
        "        \n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) \n",
        "        \n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        \n",
        "    return grad"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VMQvVGYTLABN",
        "outputId": "e3b1fa98-d14a-4dc7-b233-ca17f5f9588a"
      },
      "source": [
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "'''\n",
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    # f(x+h) 계산\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "\n",
        "    # f(x-h) 계산\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    x[idx] = tmp_val # 값 복원\n",
        "\n",
        "  return grad\n",
        "  '''\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef numerical_gradient(f, x):\\n  h = 1e-4\\n  grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\\n\\n  for idx in range(x.size):\\n    tmp_val = x[idx]\\n    # f(x+h) 계산\\n    x[idx] = tmp_val + h\\n    fxh1 = f(x)\\n\\n    # f(x-h) 계산\\n    x[idx] = tmp_val - h\\n    fxh2 = f(x)\\n\\n    grad[idx] = (fxh1 - fxh2) / (2*h)\\n    x[idx] = tmp_val # 값 복원\\n\\n  return grad\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEDIjgGrNsXQ"
      },
      "source": [
        "# 경사 하강법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1uJYqRbMVkb"
      },
      "source": [
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "  x = init_x\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f, x)\n",
        "    x -= lr * grad\n",
        "  return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4I1wN7eRU2_"
      },
      "source": [
        "# SimpleNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE3TOWMbOH1R"
      },
      "source": [
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2, 3) # 정규분포로 초기화\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIm3XQN3bNiN"
      },
      "source": [
        "# 기타 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6V8SykVbNOx"
      },
      "source": [
        "# 시그모이드 함수 정의하기\n",
        "def sigmoid(x):\n",
        "  return 1 / (1+np.exp(-x))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukWJ_dxdeLMx"
      },
      "source": [
        "# 소프트맥스 함수 정의하기\n",
        "def softmax(x):\n",
        "  exp_x = np.exp(x)\n",
        "  sum_exp_x = np.sum(exp_x)\n",
        "  return exp_x / sum_exp_x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COFas_vaqe8b"
      },
      "source": [
        "def sigmoid_grad(x):\n",
        "  return (1.0 - sigmoid(x)) * sigmoid(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyU8UgjclzVm"
      },
      "source": [
        "# 학습 구현을 위해.. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2snY-eFl8o4"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None # 손실\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t \n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_3jhyGSnwU2"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class Affine:                                              \n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "       \n",
        "        return dx\n",
        "       \n",
        "class Relu:                                                  \n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ZKQg8gRwjt"
      },
      "source": [
        "# 2층 신경망 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJvLXIuFRur0"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    #가중치 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    # 계층 생성\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "    self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y \n",
        "\n",
        "  # x : 입력 데이터, t : 정답 레이블\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1) # 최대 index의 값을 반환해준다.\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y==t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "  \n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "    grads = {}\n",
        "    \n",
        "    batch_num = x.shape[0]\n",
        "    \n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "    \n",
        "    \n",
        "    dy = (y - t) / batch_num\n",
        "    grads['W2'] = np.dot(z1.T, dy)\n",
        "    grads['b2'] = np.sum(dy, axis=0)\n",
        "    \n",
        "    da1 = np.dot(dy, W2.T)\n",
        "    dz1 = sigmoid_grad(a1) * da1\n",
        "    grads['W1'] = np.dot(x.T, dz1)\n",
        "    grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "    return grads\n",
        "    '''\n",
        "\n",
        "    # 결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "    grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "    return grads\n",
        "    '''"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hNiTbWxUT1g",
        "outputId": "c42c0ce7-a948-4f80-a155-1bbec5b0a3cb"
      },
      "source": [
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "net.params['W1'].shape # (784, 100)\n",
        "net.params['b1'].shape # (100,)\n",
        "net.params['W2'].shape # (100, 10)\n",
        "net.params['b2'].shape # (10,)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PeowoCpXo9r"
      },
      "source": [
        "# 학습 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "xVJ-4uwohUzR",
        "outputId": "c2f9ddcf-8e03-466c-e82a-5bbecd5dc852"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "\n",
        "# hyperparameters 설정\n",
        "iters_num = 10000  # 반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100      # minibatch size\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "loss = network.loss(x_batch, t_batch)\n",
        "train_loss_list.append(loss)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5ede46808518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8b9bfdb67bf0>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 최대 index의 값을 반환해준다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8b9bfdb67bf0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "LMtfSKqqU9vP",
        "outputId": "66ce7213-013d-4b98-bcc0-95eb46acd408"
      },
      "source": [
        "'''\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "# hyperparameters 설정\n",
        "iters_num = 100  # 반복 횟수\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100      # minibatch size\n",
        "learning_rate = 0.1\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # 미니배치 획득\n",
        "  batch_mask = np.random.choice(train_size, batch_size) #np.random.choice(60000, 10)은 60000 미만인 index에서 10개를 무작위로 추출\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # 기울기 계산\n",
        "  #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  # 매개변수 갱신\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  # 학습 경과 기록\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "  '''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n(x_train, t_train), (x_test, t_test) =     load_mnist(normalize=True, one_hot_label=True)\\n\\ntrain_loss_list = []\\n\\n# hyperparameters 설정\\niters_num = 100  # 반복 횟수\\ntrain_size = x_train.shape[0]\\nbatch_size = 100      # minibatch size\\nlearning_rate = 0.1\\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\\n\\nfor i in range(iters_num):\\n  # 미니배치 획득\\n  batch_mask = np.random.choice(train_size, batch_size) #np.random.choice(60000, 10)은 60000 미만인 index에서 10개를 무작위로 추출\\n  x_batch = x_train[batch_mask]\\n  t_batch = t_train[batch_mask]\\n\\n  # 기울기 계산\\n  #grad = network.numerical_gradient(x_batch, t_batch)\\n  grad = network.gradient(x_batch, t_batch)\\n\\n  # 매개변수 갱신\\n  for key in ('W1', 'b1', 'W2', 'b2'):\\n    network.params[key] -= learning_rate * grad[key]\\n\\n  # 학습 경과 기록\\n  loss = network.loss(x_batch, t_batch)\\n  train_loss_list.append(loss)\\n  \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwwgrl9GpVmg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}